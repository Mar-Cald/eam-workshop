---
title: "Intro to EAM"
author: Margherita Calderan
format:  
  revealjs:  
    width: 1200
    slide-number: true
    toc: false
    incremental: false
html-math-method:
  method: mathml
date: 2025/08/24
date-format: long  
from: markdown+emoji  
logo: img/logo.png
include-in-header:
  - text: |
      <style>
      .reveal .slide-logo {
        max-height: unset;
        height:80px;
      }
      </style>
css: styles.css
---

```{r}
#| echo: false
library(ggplot2)
library(dplyr)
```



## Decision Making {.small-text}

-   Many decisions are made rapidly and at a low cognitive level.
-   Perception triggers stored knowledge ‚Üí triggers action.
-   This process matches perception to known representations and selects a response.

:::: {.columns}

::: {.column width="70%"}

![](img/dotmot.jpg){width="700" height="400"}

:::

::: {.column width="30%" .small-small-text .center}

<br/>

> [Mulder, M. J., Wagenmakers, E. J., Ratcliff, R., Boekel, W., & Forstmann, B. U. (2012). Bias in the brain: a diffusion model analysis of prior probability and potential payoff. Journal of Neuroscience, 32(7), 2335‚Äì2343.](https://doi.org/10.1523/JNEUROSCI.4156-11.2012)

:::
:::::

## Sequential Sampling Models  

Many models assume **accumulation of noisy evidence** to thresholds. Can vary in:

-   Number of accumulators
-   Relative vs absolute decision rules
-   Constant vs time-varying drift rate
-   Stochastic vs deterministic noise
-   With/without inhibition or decay

## Wiener Process {.small-text .align-center}

The **Wiener process** (Brownian motion) is the foundation of many decision models. It models the accumulation of evidence as a noisy process:

<br/>
$$
x(t) = z + vt + sW(t)
$$
<br/>

::::: columns
::: {.column width="40%"}
-   $x(t)$: accumulated evidence at time $t$
-   z: starting point
-   v: drift rate (signal strength)
-   s: noise (standard deviation of the increments)
-   $W(t)$: standard Wiener process (Gaussian increments)

:::

::: {.column width="60%"}

```{r}
#| echo: false
#| fig-asp: .6
#| out-width: 95%
#| fig-width: 7

set.seed(11)
path = function(drift, threshold, ndt, sp1=0,  noise_constant=1, dt=0.0001, max_rt=2) {
  max_tsteps = max_rt/dt
  
  # initialize the diffusion process
  tstep = 0
  x1 = c(sp1*threshold) # vector of accumulated evidence at t=tstep
  time = c(ndt)
  
  # start accumulating
  while (x1[tstep+1] < threshold  & tstep < max_tsteps) {
    x1 = c(x1, x1[tstep+1] + rnorm(mean=drift*dt, sd=noise_constant*sqrt(dt), n=1))
    time = c(time, dt*tstep + ndt)
    tstep = tstep + 1
  }
  return (data.frame(time=time, x=x1, accumulator=rep(1, length(x1))))
}

drift = 1;threshold = 2.4; ndt = 0

sim_path = path(drift = drift,threshold = threshold, ndt = .1)

ggplot(data = sim_path, aes(x = time, y = x))+
  geom_line(size = 1, color = "blue4") +
  theme_minimal()

```
:::
:::::

## Wiener First Passage Time (FPT) {.small-text .align-center}
<br/> 
$$
d(t,x=0|\alpha,\tau,\beta,\delta) = \frac{1}{\alpha^2} \exp\left[-\alpha\beta\delta - \frac{1}{2}\delta^2(t-\tau)\right] f\left(\frac{|t-\tau|}{\alpha^2}\bigg|\beta\right)
$$ 
<br/> 

:::: {.columns}
::: {.column width="50%"}

![](img/wiener.png)

:::
::: {.column width="50%"}

>It is the expected distribution of the time until the process first hits or crosses one or the other boundary. This results in a bivariate distribution, over responses $x$ and hitting times $t$.

<br/> 
:::
::::

[Wabersich and Vandekerckhove (2014), *The R Journal*]((https://journal.r-project.org/archive/2014/RJ-2014-005/index.html))

## Full Diffusion Decision Model {.align-center}
<br/>

The **Full DDM** accounts for more behavioral phenomena by allowing **trial-to-trial variability** in key parameters:

<br/>

**Drift rate**:  $v \sim \mathcal{N}(\mu_v, \eta^2)$

**Starting point**: $z \sim \text{Uniform}(z - s_z/2, z + s_z/2)$

**Non-decision time**: $t_0 \sim \text{Uniform}(t_0 - s_{t_0}/2, t_0 + s_{t_0}/2)$

## Full DDM Parameters {.small-text .align-center}

:::: {.columns}
::: {.column width="40%"}
**a**: decision boundary

**z**: starting point

**v**: drift rate

**t_0**: non-decision time

**s**: noise scale (usually fixed)

**$\eta$**: SD of drift rate across trials

**$s_{z}$**: variability in start point

**$s_{t_0}$**: variability in non-decision time
:::
::: {.column width="60%"}
![](img/fullDDM.jpg)

:::
:::::

> [Boehm, U., Annis, J., Frank, ... & Wagenmakers, E. J. (2018). Estimating across-trial variability parameters of the Diffusion Decision Model: Expert advice and recommendations. *Journal of Mathematical Psychology*, 87, 46-75.](https://www.sciencedirect.com/science/article/pii/S002224961830021X){.small-small-text}
[Ratcliff, R., & Rouder, J. N. (1998). Modeling Response Times for Two-Choice Decisions. Psychological Science, 9(5), 347-356.](https://doi.org/10.1111/1467-9280.00067){.small-small-text}



## Purpose of Variability
<br/>

Adding variability improves the model's ability to:

-   Capture **error RT differences**
-   Reflect **trial-to-trial attention or difficulty changes**
<br/>

However, it increases computational demands.

## Racing Diffusion Model {.align-center}
<br/>

Instead of a single process choosing between boundaries, the **Racing Diffusion Model** (RDM) uses **multiple independent diffusion processes**, one per option:

<br/>

$$
dx_k(t) = v_k \, dt + s \, dW_k(t) \quad \text{for } k = 1,\dots,K
$$ <br/>

Each accumulator races toward its threshold. The first to cross **wins**.

##  {.small-small-text .align-center}

![](img/rdm.webp){width="800" height="500" fig-align="center"}

> [Tillman, G., Van Zandt, T., & Logan, G. D. (2020). Sequential sampling models without random between-trial variability: The racing diffusion model of speeded decision making. Psychonomic Bulletin & Review, 27(5), 911-936.](https://link.springer.com/content/pdf/10.3758/s13423-020-01719-6.pdf)

## RDM Characteristics {.small-text}

-   Each response has its own drift rate $v{_k}$
-   Threshold $a$ is typically shared
-   Start point $z = 0$ is often assumed
-   Noise $s$ is usually constant

<br/>
$$
f_{i}(t|b,v) = b(2 \pi t^{3})^{-\frac{1}{2}} \exp\left[-\frac{1}{2t}(v t-b)^{2}\right]
$$
<br/>
The winner is the first process to reach threshold:

<br/>

$$
g_{i}(t) = f_{i}(t) \prod\limits_{j \neq i} (1 - F_{j}(t)),
$$

## Summary

| Model        | Core Mechanism                  | Key Strengths                              |
|-------------|----------------------------------|---------------------------------------------|
| **Wiener**   | Stochastic accumulation          | Simple FPT, binary outcomes                 |
| **Full DDM** | Accumulation + param variability | Realistic RTs, error patterns               |
| **Racing DDM** | Multiple accumulators         | Handles multi-alternative decisions         |

> **Evidence accumulates noisily over time until a decision threshold is reached.**

---

## "All Models Are Wrong, but Some Are Useful"  ‚Äî G. Box {.align-center}

<br/>

Promotes understanding of behavior  
<br/>

ü™í **Occam‚Äôs Razor**: Avoid unnecessary complexity 
<br/><br/>

Accurately predicts new (**out-of-sample**) data  
<br/>

üìè Captures **uncertainty**, **variability**, and **structure**

---

## Bayesian Inference

<br/>

**Data are fixed**, but **parameters are uncertain**.

<br/>

**Bayes‚Äô Rule**:
<br/>

$$
\text{Posterior} \propto \text{Likelihood} \times \text{Prior}
$$
<br/>

- Inference produces **posterior distributions** over parameters  

- Captures uncertainty and incorporates prior knowledge  

---

## Hierarchical Modeling {.align-center}

*Are participants identical or fully independent?*

:::: {.columns}
::: {.column width="45%"}

Models **group-level structure**

each participant‚Äôs parameters ‚àº drawn from a population distribution
<br/><br/>

**Individual-level** inference  
<br/>

**Group-level** generalization
<br/>

:::

::: {.column width="55%"}


```{r}
#| echo: false
#| fig-width: 8
#| fig-asp: .7

# Function to create a normal distribution curve
normal_curve <- function(mean, sd, n = 200, range_mult = 3) {
  x <- seq(mean - range_mult*sd, mean + range_mult*sd, length.out = n)
  y <- dnorm(x, mean, sd)
  data.frame(x = x, y = y)
}

# Create data for the plots
hyper_curve <- normal_curve(0.5, 0.05)
individual_curves <- rbind(
  normal_curve(0.35, 0.06) %>% mutate(subj = "1"),
  normal_curve(0.5, 0.03) %>% mutate(subj = "2"),
  normal_curve(0.7, 0.04) %>% mutate(subj = "3")
)

# Normalize y values
max_y <- max(c(hyper_curve$y, individual_curves$y))
hyper_curve$y <- hyper_curve$y / max_y
individual_curves$y <- individual_curves$y / max_y

# Create the main plot
p1 <- ggplot() +
  # Hyper-distribution curve
  geom_line(data = hyper_curve, aes(x, y + 1.6), size = 2.5, color = "black") +
  # Individual curves
  geom_line(data = individual_curves, aes(x, y - .1, 
                                          group = subj, color = subj,
                                          fill = subj), 
            size = 2.5, show.legend = FALSE) +  # X marks
  geom_point(aes(x = c(0.38, 0.5, 0.65), y = rep(1.12, 3)), 
             shape = 21, size = 8, fill = c("cyan4", "green2", "purple2")) +
  # Labels
  annotate("text", x = 0.5, y = 1.8, label = "Œº,œÉ", color = "red", size = 10) +
  # Customize the plot
  theme_void() +
  coord_cartesian(xlim = c(0.1, 0.9), ylim = c(-0.2, 2.3), expand = FALSE) +
  # Add arrows top-down
  geom_segment(aes(x = 0.45, y = 1.6, xend = 0.39, yend = 1.2), size = 1.5,
               arrow = arrow(length = unit(0.2, "cm")), color = "gray50") +
  geom_segment(aes(x = 0.5, y = 1.6, xend = 0.5, yend = 1.2), size = 1.5,
               arrow = arrow(length = unit(0.2, "cm")), color = "gray50") +
  geom_segment(aes(x = 0.55, y = 1.6, xend = 0.64, yend = 1.2), size = 1.5,
               arrow = arrow(length = unit(0.2, "cm")), color = "gray50") +
  # Add arrows bottom-up
  geom_segment(aes(x = 0.35, y = 0.7, xend = 0.37, yend = 1.02), size = 1,
               arrow = arrow(length = unit(0.2, "cm")), color = "gray50") +
  geom_segment(aes(x = 0.5, y = 0.95, xend = 0.5, yend = 1.02), size = 1,
               arrow = arrow(length = unit(0.2, "cm")), color = "gray50") +
  geom_segment(aes(x = 0.69, y = 0.8, xend = 0.66, yend = 1.02), size = 1,
               arrow = arrow(length = unit(0.2, "cm")), color = "gray50") +
  scale_color_manual(values = c("1" = "cyan4", "2" = "green2", "3" = "purple2"))+
  geom_point(aes(x = c(0.35, 0.5, 0.7), y = rep(-0.1, 3)), 
             shape = 21, size = 8, fill = c("cyan4", "green2", "purple2"))


# Display the plot
print(p1)


```
:::
:::::


## Why Hierarchical Models?
<br/>

-   Reduces bias from unmodeled individual variability  
-   Prevents overfitting *and* underfitting  
-   **Shrinkage**: pulls extreme or low-quality estimates toward the group mean  

> [Boehm et al. (2018), *Behavior Research Methods*](https://doi.org/10.3758/s13428-018-1054-3)

::: notes
Hierarchical models aim to strike a balance between two extremes:

- **Overfitting**: Fitting each subject completely independently  
- **Underfitting**: Pooling across subjects and fitting only the average  
:::


## What Are We Trying to Infer?

We want to estimate:

- **Individual-level parameters**:  $\alpha_1, \alpha_2, \dots, \alpha_N$


- **Group-level structure**:  
  A shared population distribution  $\alpha_i \sim \mathcal{N}(\mu, \Sigma)$

This enables both **individual** and **population** inference.


## Model Structure

The hierarchical model defines a **joint posterior** over all unknowns:

$$
p(\alpha_i, \mu, \Sigma \mid \text{data}) \propto 
p(\text{data} \mid \alpha_i) \cdot 
p(\alpha_i \mid \mu, \Sigma) \cdot 
p(\mu, \Sigma)
$$

- **Likelihood**:  
  $p(\text{data} \mid \alpha_i)$  
  ‚Äî How well each participant's data is explained by their parameters

- **Group-level model**:  
  $p(\alpha_i \mid \mu, \Sigma)$  ‚Äî Individual parameters drawn from population distribution

- **Hyperprior**:  
  $p(\mu, \Sigma)$  ‚Äî Prior beliefs over the group-level structure


## Group-Level Parameters

Let $n$ = number of individual parameters per subject:

- $\mu$: vector of **population means**  
- $\Sigma$: symmetric **covariance matrix**, including:

  - Diagonal: **Variances** ($\sigma^2$)
  - Off-diagonal: **Covariances**  
    $\frac{n(n-1)}{2}$ elements total

These hyperparameters are **learned from the data**.



## Final Thoughts

- üß† Evidence Accumulation Models (EAMs) are foundational to modeling cognitive decisions  
- üìà Bayesian hierarchical inference improves parameter recovery and interpretability  
- ‚öñ Seek a balance:
  - Expressive but not too complex  
  - Flexible yet constrained  


# Thank You!

